# -*- coding: utf-8 -*-
"""Copia de EJERCICIO: Promocion de un empleado.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1IzB3rx1fkGZDFqIaiWWdodUtpC4FEMAJ

# Predecir la promoci贸n de un empleado

* Limpieza/preprocesamiento  
* Clases desbalanceadas
*   Keras-tuner

Dataset

https://www.kaggle.com/code/muhammadimran112233/employee-promotion-end-to-end-solution/input

incluir, al menos, lo siguiente


* Manejo de datos faltantes
* datos repetidos
* columnas constantes
* 驴hay outliers?
* 驴selecci贸n de atributos?
* estandarizaci贸n de datos
* Manejo de datos desbalanceados
* Keras-tuner, usar conjunto de validaci贸n
  * construir modelo con los mejores par谩metros
  * graficar loss y acc de train y validaci贸n
  * evaluar con X_test
  * obtener matriz de confusi贸n
"""

!pip install -qq keras
!pip install -qq keras_tuner
!pip install -qq imbalanced-learn

from sklearn.preprocessing import MinMaxScaler, RobustScaler
import keras_tuner as kt
import time
import seaborn as sn
from sklearn import datasets
from sklearn.metrics import  ConfusionMatrixDisplay
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import tensorflow as tf
import keras
from keras.models import Sequential
from sklearn import preprocessing
from keras.layers import Dense, Input, LeakyReLU
from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix
from tensorflow.keras.optimizers import Adam, SGD, Adagrad
from imblearn.under_sampling import RandomUnderSampler
from tensorflow.keras.callbacks import EarlyStopping
from imblearn.over_sampling import RandomOverSampler
from sklearn.utils.class_weight import compute_class_weight

"""#Dataset  """

dataset1 = pd.read_csv('/content/employee_promotion.csv')

dataset = dataset1

dataset.shape

unique_elements, count_elements = np.unique(dataset.is_promoted,return_counts=True)
print(unique_elements)
print(count_elements)
print(f"Hay un {100*count_elements[0]/count_elements.sum()}% muestras de la clase 0 (no contrata el seguro)")
print(f"Hay un {100*count_elements[1]/count_elements.sum()}% muestras de la clase 1 (contrata el seguro)")

fig1,ax1=plt.subplots()
ax1.pie( count_elements, labels=unique_elements, autopct='%1.1f%%',shadow=True,startangle=90 )
plt.savefig("pie.png")
plt.close()

"""#Limpieza del dataset"""

dataset.describe()

dataset.isnull().sum()

for column in dataset.columns.to_list():
  if dataset[column].isnull().sum() > 0:
    print(f'La columna ' + column + ' tiene ' + str(dataset[column].isnull().sum()) + ' valores perdidos')
print("Las dem谩s columnas no tienen valores perdidos")

dataset[['education']].groupby('education').value_counts()

dataset[['previous_year_rating']].groupby('previous_year_rating').value_counts()

dataset['imp_education'] = dataset['education'].isnull().astype('int')
dataset['imp_previous_year_rating'] = dataset['previous_year_rating'].isnull().astype('int')
dataset['imp_avg_training_score'] = dataset['avg_training_score'].isnull().astype('int')

dataset['education'] = dataset['education'].fillna('Bachelor\'s')
dataset['avg_training_score'] = dataset['avg_training_score'].fillna(dataset['avg_training_score'].mean().round(2))
dataset['previous_year_rating'] = dataset['previous_year_rating'].fillna(3.0)

dataset.isnull().sum()

dataset

"""#Preprocesamiento

*   codificaci贸n de atributos categ贸ricos
*   驴selecci贸n de caracter铆sticas?
* escalamiento
"""

dataset.info()

for col in ['no_of_trainings','age','previous_year_rating','length_of_service','avg_training_score']:
  mediana = dataset[col].median()
  std = dataset[col].std()

  numOutliers = 0

  for j in range( dataset.shape[0] ):
  #  print(col , j, dataset[col][j])
    if ( dataset[col].iloc[j] < mediana - 4 * std or dataset[col].iloc[j] > mediana + 4 * std ):
      numOutliers = numOutliers + 1

  if( numOutliers ):
   # ind = True
    print(f"En la columns {col} hay {numOutliers}/{dataset.shape[0]} outliers")

dataset['no_of_trainings'].value_counts()

dataset['log_no_of_trainings'] = np.log1p(dataset['no_of_trainings'])
dataset = dataset.drop('no_of_trainings',axis=1)
categorical_columns = ['department', 'region', 'education', 'gender', 'recruitment_channel']
dataset = pd.get_dummies(dataset, columns=categorical_columns, drop_first=False).astype(int)

dataset

"""NOTA: **Usar dos t茅cnicas para clases desbalanceadas**, al menos, para comparar desempe帽o

#X_train, X_test
"""

X = dataset.drop(columns=['is_promoted', 'employee_id'])

y = dataset['is_promoted'].copy()

X_train, X_test, y_train, y_test = train_test_split(X,y, stratify=y, test_size=0.2, random_state=42)

minmax = MinMaxScaler()
robust = RobustScaler()
columns_to_scale = ['age', 'previous_year_rating', 'avg_training_score']
X_train[columns_to_scale] = minmax.fit_transform(X_train[columns_to_scale])
X_train['length_of_service'] = robust.fit_transform(X_train[['length_of_service']])

X_test[columns_to_scale] = minmax.transform(X_test[columns_to_scale])
X_test['length_of_service'] = robust.transform(X_test[['length_of_service']])

print(f"X_train: hay {y_train.sum()} empleados que fueron promovidos de {X_train.shape[0]} empleados, es decir {100*y_train.sum()/X_train.shape[0]:.2}% ")
print(f"X_test: hay {y_test.sum()} empleados que fueron promovidos de {X_test.shape[0]} empleados, es decir {100*y_test.sum()/X_test.shape[0]:.2}% ")

"""#Buscando el mejor modelo"""

modelo = ['under', 'over', 'weight']

"""###RandomUnderSampler"""

rus = RandomUnderSampler( random_state = 42 )
X_resample, y_resample = rus.fit_resample(X_train, y_train)

X_resample.shape, y_resample.shape

"""###Keras-Tuner"""

def build_model(hp):
    model = Sequential()
    model.add(Input(shape=(X_train.shape[1],)))

    for i in range(hp.Int("num_layers", 1, 3)):  # Tune n煤mero de capas
        activation_choice = hp.Choice(f"activation_{i}", ["relu", "selu", "leaky_relu"])
        units = hp.Int(f'units_{i}', min_value=1, max_value=4, step=1)

        if activation_choice == "leaky_relu":
            model.add(Dense(units))
            model.add(LeakyReLU(alpha=0.1))
        else:
            model.add(Dense(units, activation=activation_choice))

    model.add(Dense(1, activation='sigmoid', name="predictions"))

    # Hiperpar谩metros del optimizador
    lr = hp.Choice('lr', values=[1e-2, 1e-3, 1e-4])
    hp_optimizers = hp.Choice('optimizer', values=["SGD", "Adam", "Adagrad"])

    optimizers_dict = {
        "Adam": Adam(learning_rate=lr, clipvalue=1.0),
        "SGD": SGD(learning_rate=lr, clipvalue=1.0),
        "Adagrad": Adagrad(learning_rate=lr, clipvalue=1.0)
    }

    model.compile(
        optimizer=optimizers_dict[hp_optimizers],
        loss="binary_crossentropy",
        metrics=['accuracy']
    )

    return model

X_resample = np.nan_to_num(X_resample)
y_resample = np.nan_to_num(y_resample)

build_model(kt.HyperParameters())

tuner = kt.Hyperband(
    build_model,
    objective            = kt.Objective("val_accuracy", "max"),
    executions_per_trial = 1,
    max_epochs           = 100,
    factor               = 3,
    directory            = 'salida',
    project_name         = 'intro_to_HP',
    overwrite            = True
)

early_stop = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)
tuner.search(X_resample, y_resample, validation_split=0.2, verbose=1, callbacks=[early_stop])

"""#Modelo"""

best_hps = tuner.get_best_hyperparameters()[0]

mi_mejor_modelo = tuner.hypermodel.build(best_hps)
mi_mejor_modelo.summary()

historial = mi_mejor_modelo.fit(X_train, y_train,validation_split=0.2,  epochs=20,  verbose=0 )

"""#Gr谩ficas

* loss
* accuracy
* matriz de confusi贸n
"""

def plot_hist_loss(hist,i):
    plt.plot(hist.history["loss"],'.r')
    plt.plot(hist.history["val_loss"],'*b')
    plt.title("model loss")
    plt.ylabel("loss")
    plt.xlabel("epoch")
    plt.legend(["train", "validation"], loc="upper left")
    plt.grid()
    plt.savefig('Perdida_'+modelo[i]+'.png')
    plt.close()

def plot_hist(hist,i):
    plt.plot(hist.history["accuracy"])
    plt.plot(hist.history["val_accuracy"])
    plt.title("model accuracy")
    plt.ylabel("accuracy")
    plt.xlabel("epoch")
    plt.legend(["train", "validation"], loc="upper left")
    plt.ylim((0,1.1))
    plt.grid()
    plt.savefig('Accuracy_'+modelo[i]+'.png')
    plt.close()

plot_hist(historial,0)
plot_hist_loss(historial,0)

X_test = np.nan_to_num(X_test)
y_test = np.nan_to_num(y_test)
y_test = y_test.reshape(-1, 1)

X_test.shape, y_test.shape

mi_mejor_modelo.evaluate(X_test, y_test)

prediccion_test = mi_mejor_modelo.predict(X_test)
prediccion_test = prediccion_test.ravel()

pred_test = np.zeros(X_test.shape[0])

for id in range(X_test.shape[0]):
    pred_test[id] = np.round( prediccion_test[id] )

con = confusion_matrix( y_test , pred_test )
disp = ConfusionMatrixDisplay( confusion_matrix = con,  display_labels = ['No-Promocion','Promocion'] ).plot()
plt.savefig('Matriz_confusion_'+modelo[0]+'.png')
plt.close()

"""
###RandomOverSampler
"""

rus = RandomOverSampler( random_state = 42 )
X_resample, y_resample = rus.fit_resample(X_train, y_train)

X_resample = np.nan_to_num(X_resample)
y_resample = np.nan_to_num(y_resample)

build_model(kt.HyperParameters())

tuner = kt.Hyperband(
    build_model,
    objective            = kt.Objective("val_accuracy", "max"),
    executions_per_trial = 1,
    max_epochs           = 10,
    factor               = 3,
    directory            = 'salida',
    project_name         = 'intro_to_HP',
    overwrite            = True
)

early_stop = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)
tuner.search(X_resample, y_resample, validation_split=0.2, verbose=1, callbacks=[early_stop])

"""#Modelo"""

best_hps = tuner.get_best_hyperparameters()[0]

mi_mejor_modelo = tuner.hypermodel.build(best_hps)
mi_mejor_modelo.summary()

historial = mi_mejor_modelo.fit(X_train, y_train,validation_split=0.2,  epochs=20,  verbose=0 )

"""#Gr谩ficas

* loss
* accuracy
* matriz de confusi贸n
"""

plot_hist(historial,1)
plot_hist_loss(historial,1)

X_test = np.nan_to_num(X_test)
y_test = np.nan_to_num(y_test)
y_test = y_test.reshape(-1, 1)

X_test.shape, y_test.shape

mi_mejor_modelo.evaluate(X_test, y_test)

prediccion_test = mi_mejor_modelo.predict(X_test)
prediccion_test = prediccion_test.ravel()

pred_test = np.zeros(X_test.shape[0])

for id in range(X_test.shape[0]):
    pred_test[id] = np.round( prediccion_test[id] )

con = confusion_matrix( y_test , pred_test )
disp = ConfusionMatrixDisplay( confusion_matrix = con,  display_labels = ['No-Promocion','Promocion'] ).plot()
plt.savefig('Matriz_confusion_'+modelo[1]+'.png')
plt.close()

"""##Weights"""

class_weights = compute_class_weight(class_weight ='balanced', classes = np.unique(y_train), y = y_train)

class_weights = dict(zip(np.unique(y_train), class_weights))
class_weights

early_stop = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)
hist = tuner.search(X_train, y_train, validation_split=0.2, class_weight=class_weights , callbacks=[early_stop])

best_hps = tuner.get_best_hyperparameters()[0]

mi_mejor_modelo = tuner.hypermodel.build(best_hps)
mi_mejor_modelo.summary()

historial = mi_mejor_modelo.fit(X_train, y_train,validation_split=0.2,  epochs=20,  verbose=0 )

plot_hist(historial,2)
plot_hist_loss(historial,2)

X_test = np.nan_to_num(X_test)
y_test = np.nan_to_num(y_test)
y_test = y_test.reshape(-1, 1)

mi_mejor_modelo.evaluate(X_test, y_test)

prediccion_test = mi_mejor_modelo.predict(X_test)
prediccion_test = prediccion_test.ravel()

pred_test = np.zeros(X_test.shape[0])

for id in range(X_test.shape[0]):
    pred_test[id] = np.round( prediccion_test[id] )

con = confusion_matrix( y_test , pred_test )
disp = ConfusionMatrixDisplay( confusion_matrix = con,  display_labels = ['No-Promocion','Promocion'] ).plot()
plt.savefig('Matriz_confusion_'+modelo[2]+'.png')
plt.close()