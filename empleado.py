# -*- coding: utf-8 -*-
"""Copia de EJERCICIO: Promocion de un empleado.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1IzB3rx1fkGZDFqIaiWWdodUtpC4FEMAJ

# Predecir la promoción de un empleado

* Limpieza/preprocesamiento  
* Clases desbalanceadas
*   Keras-tuner

Dataset

https://www.kaggle.com/code/muhammadimran112233/employee-promotion-end-to-end-solution/input

incluir, al menos, lo siguiente


* Manejo de datos faltantes
* datos repetidos
* columnas constantes
* ¿hay outliers?
* ¿selección de atributos?
* estandarización de datos
* Manejo de datos desbalanceados
* Keras-tuner, usar conjunto de validación
  * construir modelo con los mejores parámetros
  * graficar loss y acc de train y validación
  * evaluar con X_test
  * obtener matriz de confusión
"""

!pip install -qq keras
!pip install -qq keras_tuner
!pip install -qq imbalanced-learn

from sklearn.preprocessing import MinMaxScaler, RobustScaler
import keras_tuner as kt
import time
import seaborn as sn
from sklearn import datasets
from sklearn.metrics import  ConfusionMatrixDisplay
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import tensorflow as tf
import keras
from keras.models import Sequential
from sklearn import preprocessing
from keras.layers import Dense, Input, LeakyReLU
from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix
from tensorflow.keras.optimizers import Adam, SGD, Adagrad
from imblearn.under_sampling import RandomUnderSampler
from tensorflow.keras.callbacks import EarlyStopping
from imblearn.over_sampling import RandomOverSampler
from sklearn.utils.class_weight import compute_class_weight

"""#Dataset  📋"""

dataset1 = pd.read_csv('/content/employee_promotion.csv')

dataset = dataset1

dataset.shape

unique_elements, count_elements = np.unique(dataset.is_promoted,return_counts=True)
print(unique_elements)
print(count_elements)
print(f"Hay un {100*count_elements[0]/count_elements.sum()}% muestras de la clase 0 (no contrata el seguro)")
print(f"Hay un {100*count_elements[1]/count_elements.sum()}% muestras de la clase 1 (contrata el seguro)")

fig1,ax1=plt.subplots()
ax1.pie( count_elements, labels=unique_elements, autopct='%1.1f%%',shadow=True,startangle=90 )
plt.savefig("pie.png")
plt.close()

"""#Limpieza del dataset"""

dataset.describe()

dataset.isnull().sum()

for column in dataset.columns.to_list():
  if dataset[column].isnull().sum() > 0:
    print(f'La columna ' + column + ' tiene ' + str(dataset[column].isnull().sum()) + ' valores perdidos')
print("Las demás columnas no tienen valores perdidos")

dataset[['education']].groupby('education').value_counts()

dataset[['previous_year_rating']].groupby('previous_year_rating').value_counts()

dataset['imp_education'] = dataset['education'].isnull().astype('int')
dataset['imp_previous_year_rating'] = dataset['previous_year_rating'].isnull().astype('int')
dataset['imp_avg_training_score'] = dataset['avg_training_score'].isnull().astype('int')

dataset['education'] = dataset['education'].fillna('Bachelor\'s')
dataset['avg_training_score'] = dataset['avg_training_score'].fillna(dataset['avg_training_score'].mean().round(2))
dataset['previous_year_rating'] = dataset['previous_year_rating'].fillna(3.0)

dataset.isnull().sum()

dataset

"""#Preprocesamiento

*   codificación de atributos categóricos
*   ¿selección de características?
* escalamiento
"""

dataset.info()

for col in ['no_of_trainings','age','previous_year_rating','length_of_service','avg_training_score']:
  mediana = dataset[col].median()
  std = dataset[col].std()

  numOutliers = 0

  for j in range( dataset.shape[0] ):
  #  print(col , j, dataset[col][j])
    if ( dataset[col].iloc[j] < mediana - 4 * std or dataset[col].iloc[j] > mediana + 4 * std ):
      numOutliers = numOutliers + 1

  if( numOutliers ):
   # ind = True
    print(f"En la columns {col} hay {numOutliers}/{dataset.shape[0]} outliers")

dataset['no_of_trainings'].value_counts()

dataset['log_no_of_trainings'] = np.log1p(dataset['no_of_trainings'])
dataset = dataset.drop('no_of_trainings',axis=1)
categorical_columns = ['department', 'region', 'education', 'gender', 'recruitment_channel']
dataset = pd.get_dummies(dataset, columns=categorical_columns, drop_first=False).astype(int)

dataset

"""NOTA: **Usar dos técnicas para clases desbalanceadas**, al menos, para comparar desempeño

#X_train, X_test
"""

X = dataset.drop(columns=['is_promoted', 'employee_id'])

y = dataset['is_promoted'].copy()

X_train, X_test, y_train, y_test = train_test_split(X,y, stratify=y, test_size=0.2, random_state=42)

minmax = MinMaxScaler()
robust = RobustScaler()
columns_to_scale = ['age', 'previous_year_rating', 'avg_training_score']
X_train[columns_to_scale] = minmax.fit_transform(X_train[columns_to_scale])
X_train['length_of_service'] = robust.fit_transform(X_train[['length_of_service']])

X_test[columns_to_scale] = minmax.transform(X_test[columns_to_scale])
X_test['length_of_service'] = robust.transform(X_test[['length_of_service']])

print(f"X_train: hay {y_train.sum()} empleados que fueron promovidos de {X_train.shape[0]} empleados, es decir {100*y_train.sum()/X_train.shape[0]:.2}% ")
print(f"X_test: hay {y_test.sum()} empleados que fueron promovidos de {X_test.shape[0]} empleados, es decir {100*y_test.sum()/X_test.shape[0]:.2}% ")

"""#Buscando el mejor modelo"""

modelo = ['under', 'over', 'weight']

"""###RandomUnderSampler"""

rus = RandomUnderSampler( random_state = 42 )
X_resample, y_resample = rus.fit_resample(X_train, y_train)

X_resample.shape, y_resample.shape

"""###Keras-Tuner"""

def build_model(hp):
    model = Sequential()
    model.add(Input(shape=(X_train.shape[1],)))

    for i in range(hp.Int("num_layers", 1, 3)):  # Tune número de capas
        activation_choice = hp.Choice(f"activation_{i}", ["relu", "selu", "leaky_relu"])
        units = hp.Int(f'units_{i}', min_value=1, max_value=4, step=1)

        if activation_choice == "leaky_relu":
            model.add(Dense(units))
            model.add(LeakyReLU(alpha=0.1))
        else:
            model.add(Dense(units, activation=activation_choice))

    model.add(Dense(1, activation='sigmoid', name="predictions"))

    # Hiperparámetros del optimizador
    lr = hp.Choice('lr', values=[1e-2, 1e-3, 1e-4])
    hp_optimizers = hp.Choice('optimizer', values=["SGD", "Adam", "Adagrad"])

    optimizers_dict = {
        "Adam": Adam(learning_rate=lr, clipvalue=1.0),
        "SGD": SGD(learning_rate=lr, clipvalue=1.0),
        "Adagrad": Adagrad(learning_rate=lr, clipvalue=1.0)
    }

    model.compile(
        optimizer=optimizers_dict[hp_optimizers],
        loss="binary_crossentropy",
        metrics=['accuracy']
    )

    return model

X_resample = np.nan_to_num(X_resample)
y_resample = np.nan_to_num(y_resample)

build_model(kt.HyperParameters())

tuner = kt.Hyperband(
    build_model,
    objective            = kt.Objective("val_accuracy", "max"),
    executions_per_trial = 1,
    max_epochs           = 100,
    factor               = 3,
    directory            = 'salida',
    project_name         = 'intro_to_HP',
    overwrite            = True
)

early_stop = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)
tuner.search(X_resample, y_resample, validation_split=0.2, verbose=1, callbacks=[early_stop])

"""#Modelo"""

best_hps = tuner.get_best_hyperparameters()[0]

mi_mejor_modelo = tuner.hypermodel.build(best_hps)
mi_mejor_modelo.summary()

historial = mi_mejor_modelo.fit(X_train, y_train,validation_split=0.2,  epochs=20,  verbose=0 )

"""#Gráficas

* loss
* accuracy
* matriz de confusión
"""

def plot_hist_loss(hist,i):
    plt.plot(hist.history["loss"],'.r')
    plt.plot(hist.history["val_loss"],'*b')
    plt.title("model loss")
    plt.ylabel("loss")
    plt.xlabel("epoch")
    plt.legend(["train", "validation"], loc="upper left")
    plt.grid()
    plt.savefig('Perdida_'+modelo[i]+'.png')
    plt.close()

def plot_hist(hist,i):
    plt.plot(hist.history["accuracy"])
    plt.plot(hist.history["val_accuracy"])
    plt.title("model accuracy")
    plt.ylabel("accuracy")
    plt.xlabel("epoch")
    plt.legend(["train", "validation"], loc="upper left")
    plt.ylim((0,1.1))
    plt.grid()
    plt.savefig('Accuracy_'+modelo[i]+'.png')
    plt.close()

plot_hist(historial,0)
plot_hist_loss(historial,0)

X_test = np.nan_to_num(X_test)
y_test = np.nan_to_num(y_test)
y_test = y_test.reshape(-1, 1)

X_test.shape, y_test.shape

mi_mejor_modelo.evaluate(X_test, y_test)

prediccion_test = mi_mejor_modelo.predict(X_test)
prediccion_test = prediccion_test.ravel()

pred_test = np.zeros(X_test.shape[0])

for id in range(X_test.shape[0]):
    pred_test[id] = np.round( prediccion_test[id] )

con = confusion_matrix( y_test , pred_test )
disp = ConfusionMatrixDisplay( confusion_matrix = con,  display_labels = ['No-Promocion','Promocion'] ).plot()
plt.savefig('Matriz_confusion_'+modelo[0]+'.png')
plt.close()

"""
###RandomOverSampler
"""

rus = RandomOverSampler( random_state = 42 )
X_resample, y_resample = rus.fit_resample(X_train, y_train)

X_resample = np.nan_to_num(X_resample)
y_resample = np.nan_to_num(y_resample)

build_model(kt.HyperParameters())

tuner = kt.Hyperband(
    build_model,
    objective            = kt.Objective("val_accuracy", "max"),
    executions_per_trial = 1,
    max_epochs           = 10,
    factor               = 3,
    directory            = 'salida',
    project_name         = 'intro_to_HP',
    overwrite            = True
)

early_stop = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)
tuner.search(X_resample, y_resample, validation_split=0.2, verbose=1, callbacks=[early_stop])

"""#Modelo"""

best_hps = tuner.get_best_hyperparameters()[0]

mi_mejor_modelo = tuner.hypermodel.build(best_hps)
mi_mejor_modelo.summary()

historial = mi_mejor_modelo.fit(X_train, y_train,validation_split=0.2,  epochs=20,  verbose=0 )

"""#Gráficas

* loss
* accuracy
* matriz de confusión
"""

plot_hist(historial,1)
plot_hist_loss(historial,1)

X_test = np.nan_to_num(X_test)
y_test = np.nan_to_num(y_test)
y_test = y_test.reshape(-1, 1)

X_test.shape, y_test.shape

mi_mejor_modelo.evaluate(X_test, y_test)

prediccion_test = mi_mejor_modelo.predict(X_test)
prediccion_test = prediccion_test.ravel()

pred_test = np.zeros(X_test.shape[0])

for id in range(X_test.shape[0]):
    pred_test[id] = np.round( prediccion_test[id] )

con = confusion_matrix( y_test , pred_test )
disp = ConfusionMatrixDisplay( confusion_matrix = con,  display_labels = ['No-Promocion','Promocion'] ).plot()
plt.savefig('Matriz_confusion_'+modelo[1]+'.png')
plt.close()

"""##Weights"""

class_weights = compute_class_weight(class_weight ='balanced', classes = np.unique(y_train), y = y_train)

class_weights = dict(zip(np.unique(y_train), class_weights))
class_weights

early_stop = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)
hist = tuner.search(X_train, y_train, validation_split=0.2, class_weight=class_weights , callbacks=[early_stop])

best_hps = tuner.get_best_hyperparameters()[0]

mi_mejor_modelo = tuner.hypermodel.build(best_hps)
mi_mejor_modelo.summary()

historial = mi_mejor_modelo.fit(X_train, y_train,validation_split=0.2,  epochs=20,  verbose=0 )

plot_hist(historial,2)
plot_hist_loss(historial,2)

X_test = np.nan_to_num(X_test)
y_test = np.nan_to_num(y_test)
y_test = y_test.reshape(-1, 1)

mi_mejor_modelo.evaluate(X_test, y_test)

prediccion_test = mi_mejor_modelo.predict(X_test)
prediccion_test = prediccion_test.ravel()

pred_test = np.zeros(X_test.shape[0])

for id in range(X_test.shape[0]):
    pred_test[id] = np.round( prediccion_test[id] )

con = confusion_matrix( y_test , pred_test )
disp = ConfusionMatrixDisplay( confusion_matrix = con,  display_labels = ['No-Promocion','Promocion'] ).plot()
plt.savefig('Matriz_confusion_'+modelo[2]+'.png')
plt.close()